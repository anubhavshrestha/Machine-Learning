{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT97P8bneFKdT1PcX33joa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavshrestha/Machine-Learning/blob/main/Task1_Lenet_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyooONKL4Tc4"
      },
      "outputs": [],
      "source": [
        "# importing necessary modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LeNet Model\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # 10 output units for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.tanh(self.conv1(x)))\n",
        "        x = self.pool(torch.tanh(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mHOgUjCV_GmN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "total_train_samples = len(train_dataset)\n",
        "train_samples = int(0.8 * total_train_samples)\n",
        "valid_samples = total_train_samples - train_samples\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_samples, valid_samples])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "NCaqDxOVAV9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae98d61-8406-45a0-9d82-c30194642229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 40128516.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 117152508.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46529985.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 17854291.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss and optimizer\n",
        "model = LeNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "r65D6MtPAxI2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted_valid = torch.max(outputs.data, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted_valid == labels).sum().item()\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Training Loss: {running_loss / len(train_loader):.3f}, Training Accuracy: {100 * correct_train / total_train:.2f}%, Validation Loss: {val_loss / len(valid_loader):.3f}, Validation Accuracy: {100 * correct_valid / total_valid:.2f}%\")\n",
        "\n",
        "print(\"Training Finished. Now testing!\")\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4VfE4_zBWM9",
        "outputId": "9d2b2d27-a83d-49ea-e033-5d2231ca8d3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training Loss: 0.268, Training Accuracy: 92.14%, Validation Loss: 0.096, Validation Accuracy: 97.06%\n",
            "Epoch 2 - Training Loss: 0.075, Training Accuracy: 97.69%, Validation Loss: 0.076, Validation Accuracy: 97.63%\n",
            "Epoch 3 - Training Loss: 0.052, Training Accuracy: 98.38%, Validation Loss: 0.071, Validation Accuracy: 97.72%\n",
            "Epoch 4 - Training Loss: 0.039, Training Accuracy: 98.79%, Validation Loss: 0.057, Validation Accuracy: 98.21%\n",
            "Epoch 5 - Training Loss: 0.030, Training Accuracy: 99.12%, Validation Loss: 0.064, Validation Accuracy: 98.03%\n",
            "Epoch 6 - Training Loss: 0.023, Training Accuracy: 99.26%, Validation Loss: 0.052, Validation Accuracy: 98.41%\n",
            "Epoch 7 - Training Loss: 0.018, Training Accuracy: 99.45%, Validation Loss: 0.057, Validation Accuracy: 98.39%\n",
            "Epoch 8 - Training Loss: 0.016, Training Accuracy: 99.51%, Validation Loss: 0.073, Validation Accuracy: 97.93%\n",
            "Epoch 9 - Training Loss: 0.013, Training Accuracy: 99.62%, Validation Loss: 0.059, Validation Accuracy: 98.38%\n",
            "Epoch 10 - Training Loss: 0.012, Training Accuracy: 99.60%, Validation Loss: 0.049, Validation Accuracy: 98.55%\n",
            "Training Finished. Now testing!\n",
            "Accuracy on the test set: 98.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna # for hyperparameter tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlYtrrVkCLSR",
        "outputId": "8a997aec-d5a9-4fca-9562-47c050dfd5a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "y3qRAVelG9cW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    # Define and set hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)  # Learning rate in a logarithmic range\n",
        "\n",
        "    # Initialize the LeNet model\n",
        "    model = LeNet()\n",
        "\n",
        "    # Define loss and optimizer (use Adam with the suggested learning rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Return validation accuracy as Optuna aims to maximize the objective\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "GTOSjyI2HOWN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study and optimize the objective function\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_lr = best_params[\"lr\"]\n",
        "\n",
        "# Reinitialize the model with the best hyperparameters\n",
        "best_model = LeNet()\n",
        "best_optimizer = optim.Adam(best_model.parameters(), lr=best_lr)\n",
        "\n",
        "# Training loop with the best hyperparameters\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        best_optimizer.zero_grad()\n",
        "\n",
        "        outputs = best_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        best_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = best_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Training Loss: {running_loss / len(train_loader):.3f}, Validation Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Test the model with the best hyperparameters\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = best_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test set with best hyperparameters: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFen2F8MHZmR",
        "outputId": "46e87e31-3206-4055-b54a-6d382c40a7aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-30 06:53:45,545] A new study created in memory with name: no-name-02b2e8e0-5c3d-4301-88c2-0106348cc9f8\n",
            "[I 2023-10-30 06:56:33,672] Trial 0 finished with value: 0.9274166666666667 and parameters: {'lr': 0.015637274826534726}. Best is trial 0 with value: 0.9274166666666667.\n",
            "[I 2023-10-30 06:59:19,468] Trial 1 finished with value: 0.7261666666666666 and parameters: {'lr': 0.03535158759268208}. Best is trial 0 with value: 0.9274166666666667.\n",
            "[I 2023-10-30 07:02:06,400] Trial 2 finished with value: 0.8298333333333333 and parameters: {'lr': 0.030824415357067918}. Best is trial 0 with value: 0.9274166666666667.\n",
            "[I 2023-10-30 07:04:55,814] Trial 3 finished with value: 0.95475 and parameters: {'lr': 3.4004000983120935e-05}. Best is trial 3 with value: 0.95475.\n",
            "[I 2023-10-30 07:07:45,308] Trial 4 finished with value: 0.9661666666666666 and parameters: {'lr': 5.116416632880259e-05}. Best is trial 4 with value: 0.9661666666666666.\n",
            "[I 2023-10-30 07:10:34,898] Trial 5 finished with value: 0.9845833333333334 and parameters: {'lr': 0.0003280267980168418}. Best is trial 5 with value: 0.9845833333333334.\n",
            "[I 2023-10-30 07:13:32,886] Trial 6 finished with value: 0.9839166666666667 and parameters: {'lr': 0.0014698051150901826}. Best is trial 5 with value: 0.9845833333333334.\n",
            "[I 2023-10-30 07:16:31,169] Trial 7 finished with value: 0.9245833333333333 and parameters: {'lr': 1.5969528497238376e-05}. Best is trial 5 with value: 0.9845833333333334.\n",
            "[I 2023-10-30 07:19:26,883] Trial 8 finished with value: 0.9618333333333333 and parameters: {'lr': 4.0263453086463554e-05}. Best is trial 5 with value: 0.9845833333333334.\n",
            "[I 2023-10-30 07:22:13,004] Trial 9 finished with value: 0.9834166666666667 and parameters: {'lr': 0.001602899885594135}. Best is trial 5 with value: 0.9845833333333334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training Loss: 0.489, Validation Accuracy: 94.94%\n",
            "Epoch 2 - Training Loss: 0.122, Validation Accuracy: 96.87%\n",
            "Epoch 3 - Training Loss: 0.081, Validation Accuracy: 97.49%\n",
            "Epoch 4 - Training Loss: 0.063, Validation Accuracy: 97.94%\n",
            "Epoch 5 - Training Loss: 0.052, Validation Accuracy: 98.22%\n",
            "Epoch 6 - Training Loss: 0.044, Validation Accuracy: 98.36%\n",
            "Epoch 7 - Training Loss: 0.038, Validation Accuracy: 98.10%\n",
            "Epoch 8 - Training Loss: 0.033, Validation Accuracy: 98.37%\n",
            "Epoch 9 - Training Loss: 0.029, Validation Accuracy: 98.60%\n",
            "Epoch 10 - Training Loss: 0.025, Validation Accuracy: 98.55%\n",
            "Finished Training\n",
            "Accuracy on the test set with best hyperparameters: 98.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mc-Owm7DHsTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}